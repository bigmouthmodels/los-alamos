{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d9026-9e6c-4509-849f-2456ca20091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import loads\n",
    "from uuid import uuid4\n",
    "\n",
    "import duckdb\n",
    "\n",
    "from visualisations import lasagne_single, lasagne_stacked\n",
    "\n",
    "# inspect-evaldb --log_dir /home/ubuntu/los-alamos/logs --db_uri output.db --hide_manual\n",
    "con = duckdb.connect(\"gaia-t0.db\")\n",
    "\n",
    "try:\n",
    "    df = (\n",
    "        con.execute(\"\"\"\n",
    "        SELECT * FROM raw_eval_log_headers\n",
    "    \"\"\")\n",
    "        .df()\n",
    "        .assign(eval_log=lambda df: df[\"pickled_evallog\"].apply(lambda pel: loads(pel)))\n",
    "        .assign(\n",
    "            status=lambda df: df[\"eval_log\"].apply(lambda el: el.status),\n",
    "            eval_run_id=lambda df: df[\"eval_log\"].apply(lambda el: el.eval.run_id),\n",
    "            eval_task_id=lambda df: df[\"eval_log\"].apply(lambda el: el.eval.task_id),\n",
    "            eval_task=lambda df: df[\"eval_log\"].apply(lambda el: el.eval.task),\n",
    "            model=lambda df: df[\"eval_log\"].apply(lambda el: el.eval.model),\n",
    "        )\n",
    "        .drop(columns=[\"pickled_evallog\", \"eval_log\"])\n",
    "        .assign(raw_eval_log_header_uuid=lambda df: df[\"uuid\"])\n",
    "        .assign(uuid=lambda df: df[\"uuid\"].apply(lambda _: uuid4()))\n",
    "        .drop([\"inserted\"], axis=1)\n",
    "    )\n",
    "    con.execute(\"CREATE TABLE tidy_eval_log_headers AS SELECT * FROM df\")\n",
    "except duckdb.duckdb.CatalogException:\n",
    "    pass\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM tidy_eval_messages tem\n",
    "JOIN tidy_eval_sample_headers tesh ON tesh.raw_sample_uuid = tem.raw_sample_uuid\n",
    "JOIN tidy_eval_log_headers telh ON telh.raw_eval_log_header_uuid = tem.raw_log_uuid\n",
    "\"\"\").df()\n",
    "df[\"task_name\"] = df[\"target\"].apply(\n",
    "    lambda v: {\"55\": \"trench\", \"6\": \"crocodiles\", \"CUB\": \"olympics\"}[v]\n",
    ")\n",
    "\n",
    "df[\"tool_call\"] = df[\"function\"].fillna(\"Not a tool call\").astype(dtype=\"category\")\n",
    "df[\"grade\"] = df[\"scores\"].apply(lambda d: d[\"gaia_scorer\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57844fdb-7abb-4b38-abc9-abe88af0a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_order = {\n",
    "    'anthropic/claude-3-opus-20240229': 0,\n",
    "    'anthropic/claude-3-5-sonnet-20240620': 1,\n",
    "    'anthropic/claude-3-7-sonnet-20250219': 2\n",
    "}\n",
    "\n",
    "models = sorted(df[\"model\"].unique(), key=lambda v: model_to_order[v])\n",
    "tasks = df[\"task_name\"].unique()\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "df.sort_values(\n",
    "    by=\"model\",\n",
    "    key=lambda m: m.map(model_to_order)\n",
    ")\n",
    "\n",
    "for mt in product(models, tasks):\n",
    "    lasagne_single(\n",
    "        df, model=mt[0], task=mt[1], index_col=\"index\", score_col=\"tool_call\", grade_col=\"grade\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96308b-72c4-4617-8544-8589da76ff65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70794ea6-579e-4b75-8086-0b3e5c822679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
